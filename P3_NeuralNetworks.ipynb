{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "M3BrTKOwa8Lb",
    "X2URGi0bbDKP",
    "QEcdO0wmbE1-",
    "nBkueZrS9aRR",
    "2dZQVZVZwRjU",
    "yx3zDPUXU4oA",
    "MxsojjAzs3dn",
    "gA2hDCcovujy",
    "OzawZ_4kvvl3",
    "jBcUPiT-vx-g",
    "YhUpZgq5fjD3",
    "8bF5-P_Y0h9t",
    "uStwppMUFXbf",
    "nxtTGLqbFZgB",
    "lPFEpDkJFbln",
    "cVEQFf7xGypd",
    "p1FFO8IUJJud",
    "5iMQfqcc0h9t",
    "be6e0EGG0h9u",
    "-xwZAE4I0h9u",
    "3hq-iiM50h9v"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3rd practice: Neural Networks\n",
    "* **Course**: Artificial Intelligence\n",
    "* **University**: Universitat Rovira i Virgili\n",
    "\n",
    "In this practice students will explore the following fundamentals of machine learning:\n",
    "* Data analysis\n",
    "* Preprocessing\n",
    "* Basic desgin of Multi-Layer Perceptrons (MLP)\n",
    "* Training of neural networks\n",
    "* Results analysis\n",
    "* Decision making / Critical thinking\n",
    "\n",
    "While reading this notebook, please note the following information:\n",
    "* Each section (and subsection) has a title and a brief description of its contents.\n",
    "* If a section (or subsection) title starts with `Student`, the student has work to do inside it, adding code and/or text.\n",
    "* If the title of a section begins with `Teacher`, all its content (including subsections) is part of the work statement. Subsequently, it **MUST NOT** be modified.\n",
    "* Sections must be executed in order.\n",
    "* **<font color='lightgreen'>Please enter your full name in the next cell in this section and run it BEFORE going any further.</font>**\n",
    "\n",
    "## External resources\n",
    "Please, read the following tutorials to get an introduction to Google Colab and the Pandas library:\n",
    "* https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n",
    "* https://colab.research.google.com/notebooks/mlcc/intro_to_pandas.ipynb\n",
    "\n",
    "## Deliverable\n",
    "It is an **individual** practice. The student only needs to deliver **this notebook** including the answers. The file must have as name `P3_[Name]_[Surname/s].ipynb`, replacing `[Name]` and `[Surname/s]` with those from the student.\n",
    "\n",
    "## Evaluation\n",
    "To be accepted, all practices delivered must contain at least **one solution with a preprocessing step** and **one solution with a modified neural network**.\n",
    "A third solution with a second preprocessing step will be valued.\n",
    "\n",
    "<font color='orange'>**Similar or identical practices will get a grade of 0.**</font>"
   ],
   "metadata": {
    "id": "4VsFsqBkwS2u"
   }
  },
  {
   "cell_type": "code",
   "source": "STUDENT_NAME = \"GERARD PASCUAL FONTANILLES\" # @param {type:\"string\"}",
   "metadata": {
    "id": "isV-YN1wwW3d",
    "cellView": "form"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teacher: Initialization\n",
    "Defines the **Imports** and **Device** for the practice. More details in each subsection."
   ],
   "metadata": {
    "id": "M3BrTKOwa8Lb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "The following code cell determines the available packages/libraries.\n",
    "\n",
    "**You CANNOT add any other import, neither here nor anywhere else in the code.**"
   ],
   "metadata": {
    "id": "X2URGi0bbDKP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "qqAqx8e9p_yh",
    "ExecuteTime": {
     "end_time": "2024-05-03T18:35:49.065620Z",
     "start_time": "2024-05-03T18:35:47.483977Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device\n",
    "Checks if there is a GPU for training the neural networks. If not, the CPU will be used. It is recommended to first test if the preprocessing and model definition are correct in a CPU-based environment and then switch to the GPU-based environment for training."
   ],
   "metadata": {
    "id": "QEcdO0wmbE1-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "  DEVICE = \"cuda\"\n",
    "  print(\"There is GPU available. Printing GPU information:\")\n",
    "  !nvidia-smi\n",
    "else:\n",
    "  DEVICE = \"cpu\"\n",
    "  print(\"There is no GPU available, using CPU.\")"
   ],
   "metadata": {
    "id": "fV40luAAw4hf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5dc1abac-aa28-4770-a3b7-335039c11f2a",
    "ExecuteTime": {
     "end_time": "2024-05-03T18:35:54.051822Z",
     "start_time": "2024-05-03T18:35:53.558848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is GPU available. Printing GPU information:\n",
      "Fri May  3 20:35:53 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce GTX 970         Off | 00000000:29:00.0  On |                  N/A |\r\n",
      "|  0%   58C    P0              51W / 200W |    358MiB /  4096MiB |      4%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      2691      G   /usr/lib/xorg/Xorg                          161MiB |\r\n",
      "|    0   N/A  N/A      2882      G   /usr/bin/gnome-shell                         71MiB |\r\n",
      "|    0   N/A  N/A      3509      G   /usr/libexec/xdg-desktop-portal-gnome        14MiB |\r\n",
      "|    0   N/A  N/A      4284      G   ...ds/Nextcloud-3.13.0-x86_64.AppImage       19MiB |\r\n",
      "|    0   N/A  N/A      5743      G   ...ictureAPI --variations-seed-version       49MiB |\r\n",
      "|    0   N/A  N/A      6060      G   ...,262144 --variations-seed-version=1       30MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teacher: Base code\n",
    "<a name=\"base_code\"></a>\n",
    "\n",
    "Code available for use in the `Student` sections."
   ],
   "metadata": {
    "id": "nBkueZrS9aRR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading\n",
    "This loads the `CarsData.csv` file (available in Moodle) as a Pandas dataframe. **The file must first be loaded into the environment folder** ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACkAAAAqCAYAAAAu9HJYAAABEElEQVRYCWOwsLD4P9gxw2B3IMh9DC8Kuf6D8GB27KgjqRU7oyE5GpLUCgFqmTOaJkdDklohQC1zSEqTVX7q/+/n8YCrUVh1CqOfFXD9nx0h/9+SBg0Woh3pY2/8/3KWAFYH0tqhRDsywMHw/40cPjAGsdGjsilA9f+TAm68noB5BhQboFhBNwMXn2qOBFlAikNBsQKKHVwOQxanqiORDcbFJhQj2PSNOhJbqIyGJDkhMBqS1AoBapkzmrtHQxJbCBDTwIDVzcTQNKkWQQ7H11QjxmEwNTRrYGALXXqJEZ276eUgbPaMOhJbqJAjNhqS5IQaNj2jIYktVMgRGw1JckINm56hEZLYXD7YxIbEPA4AO3Sw4kFc2wQAAAAASUVORK5CYII=). If you use a web browser *other* than Google Chrome, the loading may fail (e.g., the loading circle is red and never ends).\n",
    "\n",
    "This dataset will be used for the **Manufacturer classification task**, where the machine learning model aims to predict/output the Manufacturer of a car using the rest of features as inputs.\n",
    "\n",
    "The dataset was extracted from [this website](https://www.kaggle.com/datasets/meruvulikith/90000-cars-data-from-1970-to-2024/data). The dataframe to be used in practice is a 25% subsample of the full dataset, different from that of other students."
   ],
   "metadata": {
    "id": "2dZQVZVZwRjU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "LABEL_COL_NAME = \"Manufacturer\" # Global constant\n",
    "\n",
    "def data_loading(file_name):\n",
    "  original_df = pd.read_csv(file_name)\n",
    "  original_df = original_df.drop(columns=[\"model\"]) # Model column is not available\n",
    "  original_df\n",
    "\n",
    "  # Infer column types\n",
    "  infer_type = lambda x: x if pd.api.types.is_numeric_dtype(x) else pd.Categorical(x)\n",
    "  original_df = original_df.apply(infer_type, axis=0)\n",
    "\n",
    "  # Select subset\n",
    "  frac=0.25\n",
    "  random_seed = hash(STUDENT_NAME) % (2**32-1) # Seed must be between 0 and 2**32 - 1\n",
    "  student_df = original_df.sample(frac=frac, random_state=random_seed).reset_index()\n",
    "  student_df = student_df.drop(columns=[\"index\"])\n",
    "\n",
    "  return student_df"
   ],
   "metadata": {
    "id": "CdW-8WIeuJYA"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "Functions that facilitate the data distribution analysis for the [Student: Exploratory data analysis](#data_analysis) section."
   ],
   "metadata": {
    "id": "yx3zDPUXU4oA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def feature_histogram(df, column_name):\n",
    "    column_data = df[column_name]\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(column_data):\n",
    "        plt.hist(column_data, bins=10)\n",
    "    else:\n",
    "        value_counts = column_data.value_counts()\n",
    "        value_counts.plot(kind='bar')\n",
    "\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of {}'.format(column_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def feature_stats(df, column_name, verbose=True):\n",
    "  stats = {}\n",
    "  column_data = df[column_name]\n",
    "\n",
    "  # Numerical\n",
    "  if pd.api.types.is_numeric_dtype(column_data):\n",
    "    stats[\"Min\"] = column_data.min()\n",
    "    stats[\"Max\"] = column_data.max()\n",
    "    stats[\"Mean\"] = column_data.mean()\n",
    "    stats[\"Std\"] = column_data.std()\n",
    "\n",
    "  # Categorical\n",
    "  elif pd.api.types.is_categorical_dtype(column_data):\n",
    "    stats[\"Unique\"] = column_data.value_counts()\n",
    "\n",
    "  if verbose and len(stats) > 0:\n",
    "    stats_str = \"\"\n",
    "    for name, value in stats.items():\n",
    "      stats_str += f\" {name}={value} |\"\n",
    "    print(f\"Stats of {column_name}:{stats_str}\")\n",
    "\n",
    "  return stats\n",
    "\n",
    "\n",
    "def features_relationship(df, column_name_1, column_name_2):\n",
    "    data1 = df[column_name_1]\n",
    "    data2 = df[column_name_2]\n",
    "\n",
    "    # Both columns are numerical\n",
    "    if pd.api.types.is_numeric_dtype(data1) and pd.api.types.is_numeric_dtype(data2):\n",
    "        plt.scatter(data1, data2)\n",
    "        plt.xlabel(column_name_1)\n",
    "        plt.ylabel(column_name_2)\n",
    "        plt.title('Relationship between {} and {}'.format(column_name_1, column_name_2))\n",
    "        plt.show()\n",
    "    # Both columns are categorical\n",
    "    elif pd.api.types.is_categorical_dtype(data1) and pd.api.types.is_categorical_dtype(data2):\n",
    "        cross_table = pd.crosstab(data1, data2)\n",
    "        cross_table.plot(kind='bar', stacked=True)\n",
    "        plt.xlabel(column_name_1)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Relationship between {} and {}'.format(column_name_1, column_name_2))\n",
    "        plt.show()\n",
    "    # A column is numerical and the other categorical\n",
    "    else:\n",
    "        # Check which is which\n",
    "        if pd.api.types.is_categorical_dtype(data1):\n",
    "            categorical_column = column_name_1\n",
    "            numerical_column = column_name_2\n",
    "        else:\n",
    "            categorical_column = column_name_2\n",
    "            numerical_column = column_name_1\n",
    "\n",
    "        # Get data\n",
    "        categorical_data = df[categorical_column]\n",
    "        numerical_data = df[numerical_column]\n",
    "\n",
    "        # Convert categorical data to numeric representation\n",
    "        categorical_codes = categorical_data.astype('category').cat.codes\n",
    "\n",
    "        # Plot\n",
    "        plt.scatter(numerical_data, categorical_codes)\n",
    "        plt.xlabel(numerical_column)\n",
    "        plt.ylabel(categorical_column)\n",
    "        plt.title('Relationship between {} and {}'.format(numerical_column, categorical_column))\n",
    "        plt.yticks(ticks=categorical_codes.unique(), labels=categorical_data.unique()) # Set yticks labels as category names\n",
    "        plt.show()"
   ],
   "metadata": {
    "id": "Vwbgy0_NU7b_"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch dataset\n",
    "Function that transforms the preprocessed dataframe into the training and test datasets using the PyTorch TensorDataset class. To be used in the [Experiments](#experiments) section."
   ],
   "metadata": {
    "id": "MxsojjAzs3dn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def df_to_dataset(preprocessed_df, label_col_name=LABEL_COL_NAME):\n",
    "  if not label_col_name in preprocessed_df.columns:\n",
    "    raise Exception(f\"Dataframe MUST contain the label column [{label_col_name}], preferably in the last column.\")\n",
    "\n",
    "  # Obtain inputs and labels\n",
    "  columns_to_discard = [col_name for col_name in preprocessed_df.columns if col_name == label_col_name]\n",
    "  x = preprocessed_df.drop(columns=columns_to_discard)\n",
    "  y = preprocessed_df[label_col_name]\n",
    "\n",
    "  # Encode labels (string to identifier integer)\n",
    "  label_encoder = LabelEncoder()\n",
    "  y = label_encoder.fit_transform(y)\n",
    "\n",
    "  # Convert data to PyTorch tensors\n",
    "  x_tensor = torch.tensor(x.values, dtype=torch.float)\n",
    "  y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "  # Train-test split: 80% train, 20% test\n",
    "  x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor = train_test_split(x_tensor, y_tensor, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "  # Create PyTorch datasets\n",
    "  train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "  test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "  # Get additional information for the model\n",
    "  input_size = x_train_tensor.shape[1]\n",
    "  num_classes = len(label_encoder.classes_)\n",
    "\n",
    "  return train_dataset, test_dataset, input_size, num_classes, label_encoder"
   ],
   "metadata": {
    "id": "d33JkXNl79nY"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "Basic function for the model creation and its movement to the corresponding DEVICE (GPU or CPU). To be used in the [Experiments](#experiments) section."
   ],
   "metadata": {
    "id": "gA2hDCcovujy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model(model_class, input_size, num_classes, verbose=True):\n",
    "  model = model_class(input_size, num_classes)\n",
    "  model.to(DEVICE)\n",
    "\n",
    "  if verbose:\n",
    "    num_parameters = 0\n",
    "    for layer in model.parameters():\n",
    "      num_parameters += layer.nelement() * layer.element_size()\n",
    "    print(f\"The model has {num_parameters} parameters\")\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "id": "h4PJJ85gqhJ5"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Function that trains a **model** with a **dataset**, for a **number of epochs** and using a specific **learning rate (lr)**. To be used in the [Experiments](#experiments) section."
   ],
   "metadata": {
    "id": "OzawZ_4kvvl3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, dataset, num_epochs, lr):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create PyTorch dataloader\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Set up loss evolution plot\n",
    "    losses = []\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss Evolution')\n",
    "    line, = ax.plot([], [])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to GPU\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # Update lists for loss plotting\n",
    "        mean_epoch_loss = epoch_loss / num_batches\n",
    "        losses.append(mean_epoch_loss)\n",
    "\n",
    "        # Update plot\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(1, len(losses)+1))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        # Print specific epoch info\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Mean epoch loss: {mean_epoch_loss:.4f}')\n",
    "\n",
    "    # Turn off interactive mode and show plot\n",
    "    plt.ioff()\n",
    "    plt.close()\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "id": "Z0AXXQK635dI"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "Functions for the result analysis of the [Experiments](#experiments) section. In particular, for the analysis of [classification accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy?hl=es-419) and [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n"
   ],
   "metadata": {
    "id": "jBcUPiT-vx-g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, dataset):\n",
    "  # Create dataloader\n",
    "  eval_loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "  with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      all_targets = np.empty(len(dataset), dtype=int)\n",
    "      all_predicted = np.empty_like(all_targets)\n",
    "      for inputs, targets in eval_loader:\n",
    "          # Move data to GPU\n",
    "          inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "          # Forward pass\n",
    "          outputs = model(inputs)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "          # Store targets and predicted labels\n",
    "          all_targets[total:total+targets.size(0)] = targets.cpu().numpy()\n",
    "          all_predicted[total:total+targets.size(0)] = predicted.cpu().numpy()\n",
    "\n",
    "          # Update total and correct predictions\n",
    "          total += targets.size(0)\n",
    "          correct += (predicted == targets).sum().item()\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "  return accuracy, all_targets, all_predicted\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(all_targets, all_predicted, label_encoder):\n",
    "  # Calculate confusion matrix\n",
    "  cm = confusion_matrix(all_targets, all_predicted)\n",
    "\n",
    "  # Plot confusion matrix\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "  plt.xlabel('Predicted labels')\n",
    "  plt.ylabel('True labels')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "Yvsz3pZ9qi6V"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Student: Exploratory data analysis\n",
    "<a name=\"data_analysis\"></a>\n",
    "\n",
    "In this section, the student will have to perform the following tasks:\n",
    "* Load the dataset.\n",
    "* Analyze each feature's data distribution using plots, Pandas code (if needed) and textual reasoning (approximately one paragraph per feature) about them.\n",
    "* Analyze relevant relationships between pairs of features' data distributions with plots, Pandas code (if needed) and textual reasoning (approximately one paragraph per relationship) about them. Focus only on the important pairs to avoid overwhelming combinations."
   ],
   "metadata": {
    "id": "YhUpZgq5fjD3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: YOUR CODE HERE"
   ],
   "metadata": {
    "id": "64hxfYsTxr0f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teacher: Baseline solution\n",
    "<a name=\"baseline\"></a>\n",
    "This defines the most basic (and unsuccessful) ready-to-use solution for the **Manufacturer classification task**. It includes the main components:\n",
    "* Preprocessing\n",
    "* Model\n",
    "* Training\n",
    "* Evaluation\n",
    "\n",
    "Nonetheless, there are a lot of important things missing, as can be seen from the very poor results. Use this as inspiration for your [Solution 1A](#solution_1a), that **must** be better than this."
   ],
   "metadata": {
    "id": "8bF5-P_Y0h9t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "id": "uStwppMUFXbf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocessing_0(df):\n",
    "  preprocessed_df = df.copy()\n",
    "\n",
    "  # Define columns to use and categorical\n",
    "  categorical_cols = [\"transmission\", \"fuelType\"]\n",
    "  columns_to_use = [\"year\", \"price\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"] + categorical_cols\n",
    "\n",
    "  # Select columns to use\n",
    "  preprocessed_df = preprocessed_df[columns_to_use]\n",
    "\n",
    "  # Column transformer of the features\n",
    "  column_transformer = ColumnTransformer([('onehot', OneHotEncoder(), categorical_cols)],\n",
    "                                         remainder='passthrough')\n",
    "  preprocessed_df = pd.DataFrame(column_transformer.fit_transform(preprocessed_df))\n",
    "  # IMPORTANT: Student's can't use ColumnTransformer or similars, only Pandas functions\n",
    "\n",
    "  # Label column at the end\n",
    "  preprocessed_df[LABEL_COL_NAME] = df[LABEL_COL_NAME]\n",
    "\n",
    "  return preprocessed_df\n",
    "\n",
    "preprocessed_df = preprocessing_0(df)\n",
    "train_dataset, test_dataset, input_size, num_classes, label_encoder = df_to_dataset(preprocessed_df)"
   ],
   "metadata": {
    "id": "fQhqKKIt0h9t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "id": "nxtTGLqbFZgB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Model_0(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Model_0, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, num_classes),\n",
    "            nn.Softmax(dim=1) # Mandatory activation to normalize probabilities between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = create_model(Model_0, input_size, num_classes)"
   ],
   "metadata": {
    "id": "lMfKqYDD0h9t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "id": "lPFEpDkJFbln"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "losses = train(model, train_dataset, num_epochs=5, lr=0.001)"
   ],
   "metadata": {
    "id": "hfENvrBr0h9u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "id": "cVEQFf7xGypd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_accuracy, _, _ = evaluate(model, train_dataset)\n",
    "test_accuracy, test_all_targets, test_all_predicted = evaluate(model, test_dataset)\n",
    "\n",
    "print(f'Train accuracy: {train_accuracy*100:.2f}%')\n",
    "print(f'Test accuracy: {test_accuracy*100:.2f}%')\n",
    "\n",
    "plot_confusion_matrix(test_all_targets, test_all_predicted, label_encoder)"
   ],
   "metadata": {
    "id": "nDHtcS7tGfBf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Student: Experiments\n",
    "<a name=\"experiments\"></a>\n",
    "\n",
    "In this section, the student will have to create two or three consequent solutions for the **Manufacturer classification task**. Each solution must contain the following components:\n",
    "* **Idea**: What is the target/reasoning of the solution (e.g., I observed that the previous model had a very reduced size and I want to explore the effects of using a bigger model).\n",
    "* **Preprocessing**: Prepare the desired features of your dataframe for the neural network. To this end, students can <font color='orange'>**only use Pandas' functions**</font>.\n",
    "* **Model**: Definition of the Multi-Layer Perceptron model, **only** using [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers and\n",
    " [activations from this list](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n",
    "* **Training**: Perform the learning process trying to maximize the results.\n",
    "* **Evaluation**: Measuring the performance of the trained model.\n",
    "* **Results analysis**: Examination of the solution, focusing in:\n",
    "  * Training\n",
    "  * Accuracy\n",
    "  * Confusion matrix\n",
    "  * Training time\n",
    "\n",
    "It can be seen that these are the same as for the baseline solution, but adding the **Idea** and **Results analysis** components. An extended analysis of the baseline results will also be requested.\n",
    "\n",
    "None of the solutions has to be the best/ideal, but all of them must be better than the [baseline](#baseline). The idea is to present consequent solutions, being each one the coherent next step of the previous one (the previous step of [Solution 1A](#solution_1a) is the [baseline](#baseline)). This does not imply that each solution has to improve the results of the previous one, but to modify at least one of the components (i.e., preprocessing or model) in a significant and coherent way.\n",
    "\n",
    "In particular, we aim for the first two solutions ([Solution 1A](#solution_1a) and [Solution 1B](#solution_1b)) to emphasize preprocessing. Specifically, both will utilize the same basic MLP model (better than that of the [baseline](#baseline)) but experiment with two distinct preprocessing approaches. An explicit comparison of these preprocessing methods and their respective outcomes is necessary. For [Solution 2](#solution_2), the preprocessing method from one of the earlier solutions will be employed. The goal of this final solution is to enhance results by improving/refining the MLP model."
   ],
   "metadata": {
    "id": "WEdi-JcS0h9s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis of baseline\n",
    "Provide a comprehensive analysis of the [baseline solution](#baseline). Apart from the results (i.e., **training, accuracy, confusion matrix and runtime**), a review of the solution design (i.e., **preprocessing and model**) is also required. It is not necessary to explain or analysis the code, but the choices. For instace, which features are used and how."
   ],
   "metadata": {
    "id": "p1FFO8IUJJud"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: YOUR ANALYSIS HERE"
   ],
   "metadata": {
    "id": "MZ9k9S4uJMRy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Common functions\n",
    "For avoiding code repetition along all the solutions, you can use this subsection for your common functions and/or classes."
   ],
   "metadata": {
    "id": "5iMQfqcc0h9t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: YOUR CODE HERE"
   ],
   "metadata": {
    "id": "uA7dXeUIxyh2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution 1A: First preprocessing\n",
    "<a name=\"solution_1a\"></a>\n",
    "\n",
    "Subsequent of [Baseline](#baseline), it implements a basic MLP (better than the provided at the baseline) and a first option for the preprocessing step."
   ],
   "metadata": {
    "id": "be6e0EGG0h9u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: YOUR CODE HERE"
   ],
   "metadata": {
    "id": "oZJDOanXx8YA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution 1B: Second preprocessing solution\n",
    "<a name=\"solution_1b\"></a>\n",
    "\n",
    "Subsequent of the [Solution 1A](#solution_1a), uses the same basic MLP but with an alternative preprocessing step."
   ],
   "metadata": {
    "id": "-xwZAE4I0h9u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: YOUR CODE HERE"
   ],
   "metadata": {
    "id": "TOhUVc6lx9Kh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution 2: New model solution\n",
    "<a name=\"solution_2\"></a>\n",
    "\n",
    "Subsequent of either [Solution 1A](#solution_1a) or [Solution 1B](#solution_1b), uses the same preprocessing step but improves the MLP model."
   ],
   "metadata": {
    "id": "3hq-iiM50h9v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: YOUR CODE HERE"
   ],
   "metadata": {
    "id": "jDpa4tmwx9jI"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
